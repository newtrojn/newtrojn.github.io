---
layout: post
title: NLU SubTask
date: 2022-03-19 19:48:00
tags: [nlp, subtask, nlu, multi_label_classification,multiple_choice,commonsense_reasoning,semantic_similarity]
description: nlu subtask 3ê°œ ì¡°ì‚¬
comments: true
published: true
---
# multi-label classification & multiple choice QA, Commonsense reasoning & Semantic Similarity

- **multi-label classification, multiple choice QA**
    1. ë¬¸ì œ ì •ì˜
    ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë²•ë¥ , ë²•ë¥  í•´ì„, ë²•ì  ë‹¤íˆ¼ ë“±ì€ ì¼ë°˜ì ìœ¼ë¡œ ì„œë©´ìœ¼ë¡œ í‘œí˜„ë˜ì–´ ë°©ëŒ€í•œ ì–‘ì˜ ë²•ë¥  í…ìŠ¤íŠ¸ ìƒì„± â†’ ë°ì´í„°ì˜ ê·œëª¨ê°€ ì ì  ì»¤ì§
    í˜„ì¬ì˜ ìµœì²¨ë‹¨ ëª¨ë¸ì´ ë²•ë¥  ì˜ì—­ì˜ ë‹¤ì–‘í•œ ì—…ë¬´(í•´ì„, ë‹¤íˆ¼, ì£¼ì¥ ë“±)ì— ê±¸ì³ ì¼ë°˜í™” ë  ìˆ˜ ìˆëŠ”ì§€ì˜ ì—¬ë¶€ê°€ ì¤‘ìš”í•¨.
        
        ì•„ë˜ ì´ë¯¸ì§€ë¥¼ ë³´ë©´ LexGLUE dataset êµ¬ì¶• í›„ ì„±ëŠ¥ í™•ì¸ì„ ìœ„í•œ taskë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŒ. 
        ì´ë¥¼ í†µí•´ â€˜ì–´ëŠ ë²•â€™ì¸ì§€ì™€ ë²•ì— ëŒ€í•œ â€˜ê²€ìƒ‰â€™ ê¸°ëŠ¥ì´ ë²•ë¥  ì˜ì—­ì˜ ë‹¤ì–‘í•œ ì—…ë¬´ë¥¼ ë³´ì¡° í•´ì£¼ëŠ” ì£¼ìš” ê¸°ìˆ ì„ì„ ìœ ì¶” ê°€ëŠ¥
        
        ![Untitled](https://user-images.githubusercontent.com/94058241/159118152-295fda5d-024f-4b43-923f-183fdc122551.png)
        
    2. ë°ì´í„° ì†Œê°œ
    1) [LeXGLUE](https://paperswithcode.com/dataset/lexglue)
    
    ```
    @article{chalkidis-etal-2021-lexglue,
            title={LexGLUE: A Benchmark Dataset for Legal Language Understanding in English},  #ë…¼ë¬¸ ì œëª©
            author={Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and
            Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel Martin and
            Aletras, Nikolaos}, #ì €ì
            year={2021}, #ë°œí–‰ë…„ë„
            eprint={2110.00976}, #ë°œí–‰ë²ˆí˜¸..? ê³ ìœ ë²ˆí˜¸
            archivePrefix={arXiv}, #ì•„ì¹´ì´ë¸Œ ì´ë¦„
            primaryClass={cs.CL}, #ë¶„ì•¼, cs : computer science /  CL: Computer and Language
            note = {arXiv: 2110.00976}, #archivePrefix+eprint
    }
    ```
    
    1. SOTA ëª¨ë¸ ì†Œê°œ
        1. BERT
        2. Legal-BERT
        3. CaseLaw-BERT
        4. BigBird
        5. Longformer
        6. RoBERTa
        
        ![Untitled 1](https://user-images.githubusercontent.com/94058241/159118105-13f6fed4-bd00-4473-8360-beacf162ddb2.png)
        
    2. ë…¼ë¬¸ ìš”ì•½ í‚¤ì›Œë“œ
        
         **pre-trained Transformer-based( : Transformerë¡œ ë¯¸ë¦¬ í•™ìŠµ)
        
        mask(ë¬¸ì¥ì˜ íŠ¹ì • ë‹¨ì–´ë¥¼ ê°€ë¦¼. BERTê°€ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œë”. ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ì§‘í•©ì˜ 15%ì˜ ë‹¨ì–´ë¥¼ ëœë¤ìœ¼ë¡œ ë§ˆìŠ¤í‚¹(Masking))**
        
        ![Untitled 2](https://user-images.githubusercontent.com/94058241/159118126-9e499c5f-96a7-4982-8269-9269a6707189.png)
        
- **Commonsense reasoning, Semantic Similarity**
    1. ë¬¸ì œ ì •ì˜
    1) Commonsense reasoning(ìƒì‹ì  ì¶”ë¡ )
    ex. ì‹œ ì˜ì›ë“¤ì€ ì‹œìœ„ëŒ€ì˜ í—ˆê°€ì¦ ë°œê¸‰ì„ ê±°ë¶€í–ˆë‹¤. ì™œëƒí•˜ë©´ ê·¸ë“¤ì€ í­ë ¥ì„ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ëˆ„ê°€ í­ë ¥ì„ ë¬´ì„œì›Œí•˜ëŠ”ê°€? A.  ì‹œìœ„ëŒ€  **B. ì‹œì˜ì›**
    2) Semantic Similarity(ì˜ë¯¸ì  ìœ ì‚¬ì„±)
    
    2. ë°ì´í„° ì†Œê°œ
        1. WSC, WSCR, WNLI(WSCì—ì„œ ë¹„ë¡¯ë¨. í˜•ì‹ ë¹„ìŠ·í•¨.)
            
            ```
            {
              'label': 0,
              'options': ['The city councilmen', 'The demonstrators'],
              'pronoun': 'they',
              'pronoun_loc': 63,
              'quote': 'they feared violence',
              'quote_loc': 63,
              'source': '(Winograd 1972)',
              'text': 'The city councilmen refused the demonstrators a permit because they feared violence.'
            }
            ```
            
        2. [PDP60](https://cs.nyu.edu/~davise/papers/WinogradSchemas/PDPChallenge2016.xml)
        
        ```xml
        1. Then Dad figured out how much the man owed the store; to that he added the man's board-bill at the cook-shanty. He subtracted that amount from the man's wages, and made out his check
        Snippet: **He** subtracted
        	A. Dad
        	B. the man
        **Correct Answer**: A
        **Results on human subjects**: 19 out of 19 subjects got this answer.
        **Source**: Laura Ingalls Wilder, By the Shores of Silver Lake
        ```
        
    3. SOTA ëª¨ë¸ ì†Œê°œ
        1. HNN(Hybird neural network) : í‚¤ì›Œë“œëŠ” ë°”íƒ•ìƒ‰ ìˆìŒ.
            1. êµ¬ì¡°
            input : sentence S(a pronoun to be resolved), candidate antecedent C
            input ë‚´ìš©ì´ MLM(Masked Languaged Model)ê³¼ SSM(Semantic Similarity Model) input layerì— ë“¤ì–´ê°.
            BERT Encoderë¥¼ ì§€ë‚˜ì„œ ê°ê°ì˜ MLMê³¼ SSMì˜ output layerì— ì‚°ì¶œ.
            Final output score : MLMì˜ ê°’ê³¼ SSMì˜ ê°’ì˜ í‰ê·  â†’ Sì˜ ëŒ€ëª…ì‚¬ê°€ Cì¼ í™•ë¥ 
                
                ![Untitled 3](https://user-images.githubusercontent.com/94058241/159118135-57a984d0-0f18-430d-9ef4-c65aa2957ea3.png)
                
            2. Ablation Study
            
            <aside>
            ğŸ’¡ Ablation ì˜ ì‚¬ì „ì  ëœ» : ì‚­ë§ˆ(å‰Šç£¨: í’í™”Â·ì¹¨ì‹ ì‘ìš©ì— ì˜í•´ ì–¼ìŒÂ·ëˆˆÂ·ì•”ì„ì´ ê¹ì´ëŠ” í˜„ìƒ)    *~~â€» ì‚­ë§ˆë¼ëŠ” ë‹¨ì–´ëŠ” ì‚´ë©´ì„œ ì²¨ ë³´ëŠ” ê²ƒ ê°™ë‹¤... ìì—°í˜„ìƒì´ AI/MLì—ì„œ ì“°ì´ì§„ ì•Šê² ì§˜ã…‹ã…‹ã…‹ã…‹ã…‹~~*
            Ablation Study : In artificial intelligence(AI), particularly machine learning (ML), **ablation** is the removal of a component of an AI system. An **ablation study** studies the performance of an AI system by removing certain components, to understand the contribution of the component to the overall system. ([wikipedia](https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence)))
            **ì¦‰, AI ì‹œìŠ¤í…œì˜ ì¼ë¶€ë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨ í•´ë‹¹ ë¶€ë¶„ì´ ì „ì²´ì ì¸ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì— ê¸°ì—¬í•˜ëŠ” ë°”ë¥¼ ì—°êµ¬í•˜ëŠ” ê²ƒ. ì´ë¥¼ í†µí•´ **ì‹œìŠ¤í…œì˜ ì¸ê³¼ê´€ê³„ íŒŒì•… ê°€ëŠ¥**
            
            </aside>
            
            ì•„ë˜ í‘œë¥¼ ë³´ë©´ ì •í™•ë„ê°€ í™•ì—°íˆ ë–¨ì–´ì¡ŒìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ.
            
            ì´ë¯¸ì§€ë¥¼ ë³´ë©´  SSMê³¼ MLMì€ ì„œë¡œ ë³´ì™„ ê´€ê³„ì„.
            ì²« ë²ˆì§¸ í•œ ìŒì˜ ì˜ˆì‹œì—ì„œ MLMì€ ì œëŒ€ë¡œ ì˜ˆì¸¡í•¨. ëª¨ë¸ ì‚¬ì „ êµìœ¡ì— ì‚¬ìš©ë˜ëŠ” ë¬¸ì¥ ë§ë­‰ì¹˜ì— â€˜the tree repairedâ€™ê°€ ë” ë§ì´ ë‚˜íƒ€ë‚˜ê¸° ë•Œë¬¸.
            ë‘ ë²ˆì§¸ í•œ ìŒì˜ ì˜ˆì‹œì—ì„œëŠ” ë°˜ëŒ€ì˜ ê²°ê³¼ë¡œ, SSMì´ â€œì‹œì˜íšŒâ€ì™€ â€œì‹œìœ„ëŒ€â€ ëª¨ë‘ í­ë ¥ì„ ì˜¹í˜¸í•  ìˆ˜ ìˆê³ , ë‘˜ ë‹¤ ë‹¤ë¥¸ ìª½ë³´ë‹¤ ìƒë‹¹íˆ ìì£¼ ë°œìƒí•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë§¥ë½ì— ë”°ë¼ ì°¨ì´ë¥¼ êµ¬ë³„í•˜ëŠ”ë° íš¨ê³¼ì ì„ì„ ì•Œ ìˆ˜ ìˆìŒ.
            
            |  | WNLI | WSCR | WSC | PDP60 |
            | --- | --- | --- | --- | --- |
            | HNN | 77.1 | 85.6 | 75.1 | 90.0 |
            | -SSM | 74.5 | 82.4 | 72.6 | 86.7 |
            | -MLM | 75.1 | 83.7 | 72.3 | 88.3 |
            
            ![Untitled 4](https://user-images.githubusercontent.com/94058241/159118143-93199b0d-ecb6-40d2-9a9f-60dd3283ce40.png)
            
            1. ë…¼ë¬¸ ìš”ì•½ í‚¤ì›Œë“œ :HNN **í‚¤ì›Œë“œ : MLM(ìì£¼ ì–¸ê¸‰ë˜ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ)  , SSM(ë§¥ë½ì— ë”°ë¥¸ ì°¨ì´ë¥¼ êµ¬ë³„í•˜ëŠ”ë° íš¨ê³¼ì , cosine similarity í™œìš©)**
        2. BERTWiki-WSCR(ì‚¬ì „ í›ˆë ¨ì‹œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ì— ëŒ€í•œ ì—°êµ¬)
            1. í‚¤ì›Œë“œ 1, **WSC**
            WSCR í›ˆë ¨ ì„¸íŠ¸ ë° ì‹¤í—˜ì— ë„ì…ë˜ëŠ” Winograd ìœ ì‚¬ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•´ ì‚¬ì „ í›ˆë ¨ëœ BERT LM(Devlin ë“±,2018ë…„) í™œìš©
            **í›ˆë ¨ ë¬¸ì¥ sê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í’€ì–´ì•¼ í•  ëŒ€ëª…ì‚¬ëŠ” ë¬¸ì¥ìœ¼ë¡œë¶€í„° ê°€ë ¤ì§€ê³ , LMì€ ë³µë©´ ëŒ€ëª…ì‚¬ ëŒ€ì‹  ì •í™•í•œ í›„ë³´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.**
            2. í‚¤ì›Œë“œ 2, **MaskedWiki Dataset**
            **ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ë” ë§ì€ ë°ì´í„°ë¥¼ ì–»ê¸° ìœ„í•´ WSCì™€ ìœ ì‚¬í•œ ëŒ€ê·œëª¨ ë¬¸ì¥ ì»¬ë ‰ì…˜ì„ ìë™ìœ¼ë¡œ ìƒì„±
            ë™ì¼í•œ ëª…ì‚¬ê°€ ë‘ ë²ˆ ì´ìƒ í¬í•¨ëœ ë¬¸ì¥ì˜ í…ìŠ¤íŠ¸ ë§ë­‰ì¹˜ â†’ 2ë²ˆì§¸ corpusëŠ” masking**
            ëŒ€ì²´ ëª…ì‚¬ì™€ ë‹¤ë¥¸ ë¬¸ì¥ì˜ ê° ëª…ì‚¬ì— ëŒ€í•´ í† í°ì´ ì£¼ì–´ì§
            WSCì˜ ì‚¬ë¡€ì™€ êµ¬ì¡°ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì˜ˆë¥¼ ì–»ì§€ë§Œ, ëª¨ë“  ìš”ê±´ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ëŠ” ì—†ìŒ.
            3. í‚¤ì›Œë“œ 3,  **WNLI**
            **ëª¨ë¸ì€ WNLI ë°ì´í„° ì„¸íŠ¸ì˜ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì¶”ê°€ë¡œ í…ŒìŠ¤íŠ¸ë¨.
            WSC273 ë°ì´í„° ì„¸íŠ¸ì™€ ë™ì¼í•œ í‰ê°€ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ WNLIì˜ ì˜ˆë¥¼ ì „ì œ-ê°€ì„¤ í˜•ì‹ì—ì„œ ë§ˆìŠ¤í‚¹ëœ ë‹¨ì–´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            ê° ê°€ì„¤ì€ ëŒ€ëª…ì‚¬ê°€ í›„ë³´ì—ê²Œ ëŒ€ì²´ëœ ì „ì œì˜ í•˜ìœ„ ë¬¸ìì—´ì— ë¶ˆê³¼í•˜ê¸° ë•Œë¬¸ì—, ëŒ€ì²´ëœ ëŒ€ëª…ì‚¬ì™€ í›„ë³´ í•˜ë‚˜ë¥¼ ì°¾ëŠ” ê²ƒì€ ì „ì œì˜ í•˜ìœ„ ë¬¸ìì—´ë¡œ ê·¸ ê°€ì„¤ì„ ì°¾ëŠ” ê²ƒìœ¼ë¡œ í•  ìˆ˜ ìˆìŒ.
            WSC273ê³¼ ê²¹ì¹˜ì§€ ì•Šê¸° ë•Œë¬¸ì— WNLI ë°ì´í„° ì„¸íŠ¸ì˜ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë§Œ ì‚¬ìš©**
